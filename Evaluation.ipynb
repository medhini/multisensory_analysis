{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py  \n",
    "import numpy as np\n",
    "import os, sys\n",
    "from scipy.misc import imresize\n",
    "import cv2\n",
    "import random\n",
    "import soundfile as sf\n",
    "import category_getter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, train, frames_len=40, transform=None, h5_file='/media/jeff/Backup/CS598PS/data_2682.h5', transform_label=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            train (bool): Whether or not to use training data\n",
    "            frames (int): Number of video frames per video sample\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.frames_len = frames_len\n",
    "        \n",
    "        if self.train:\n",
    "            dataset = h5py.File(h5_file,'r')\n",
    "            self.videos_train = np.array(dataset['videos_train'])\n",
    "            self.sounds_train = np.array(dataset['sounds_train'])\n",
    "        else:\n",
    "            dataset = h5py.File(h5_file,'r')\n",
    "            self.videos_test = np.array(dataset['videos_test'])\n",
    "            self.sounds_test = np.array(dataset['sounds_test'])\n",
    "#             self.filenames_test = np.load('/media/jeff/Backup/CS598PS/test_filenames.npy')\n",
    "        dataset.close()\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.videos_train)\n",
    "        return len(self.videos_test)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.train:\n",
    "            image = self.videos_train[idx]\n",
    "            audio = self.sounds_train[idx]\n",
    "        else:\n",
    "            image = self.videos_test[idx]\n",
    "            audio = self.sounds_test[idx]\n",
    "#             filename = self.filenames_test[idx]\n",
    "\n",
    "        # Randomly sample 4 seconds from 10 second clip\n",
    "        if random.random() < 0.5:\n",
    "            start = random.randint(0,10) # Start frame\n",
    "        else:\n",
    "            start = random.randint(50,60)\n",
    "        new_image = np.zeros((self.frames_len,256,256,1), dtype=np.uint8)\n",
    "        for i in range(self.frames_len):\n",
    "            new_image[i] = np.expand_dims(image[start+i],2)\n",
    "        \n",
    "        # Randomly align or misalign audio sample\n",
    "        if random.random() < 0.5: # align\n",
    "            audio = audio[int(start*220500/100.0):int(start*220500/100.0)+88200]\n",
    "            label = 0\n",
    "        else: # misalign\n",
    "            if start < 30: # Add shift\n",
    "                shift = random.randint(20, 60-start) # frame shift amount\n",
    "                start = start+shift\n",
    "            else: # Subtract shift\n",
    "                shift = random.randint(20, start) # frame shift amount\n",
    "                start = start-shift\n",
    "            audio = audio[int(start*220500/100.0):int(start*220500/100.0)+88200]\n",
    "            label = 1\n",
    "            \n",
    "        transform_image = np.zeros((self.frames_len,1,224,224))\n",
    "        if self.transform:\n",
    "            for i in range(self.frames_len):\n",
    "                transform_image[i] = self.transform(new_image[i]) # Transform image frames\n",
    "        \n",
    "        return (transform_image, audio, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Block2(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, downsample=None):\n",
    "        super(Block2, self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding=0, dilation=1, groups=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=1, stride=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Block3(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=(1,1,1), stride=1, downsample=None, padding=0):\n",
    "        super(Block3, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding=padding, dilation=1, groups=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=(1,1,1), stride=1)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "def Linear(in_features, out_features, dropout=0.):\n",
    "    m = nn.Linear(in_features, out_features)\n",
    "    m.weight.data.normal_(mean=0, std=math.sqrt((1 - dropout) / in_features))\n",
    "    m.bias.data.zero_()\n",
    "    return nn.utils.weight_norm(m)\n",
    "\n",
    "class alignment(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(alignment, self).__init__()\n",
    "        \"\"\"Sound Features\"\"\"\n",
    "        self.conv1_1 = nn.Conv1d(2, 64, 65, stride=4, padding=0, dilation=1, groups=1, bias=True)\n",
    "        self.pool1_1 = nn.MaxPool1d(4, stride=4)\n",
    "\n",
    "        self.s_net_1 = self._make_layer(Block2, 64, 128, 15, 4, 1)\n",
    "        self.s_net_2 = self._make_layer(Block2, 128, 128, 15, 4, 1)\n",
    "        self.s_net_3 = self._make_layer(Block2, 128, 256, 15, 4, 1)\n",
    "        \n",
    "        self.pool1_2 = nn.MaxPool1d(3, stride=3)\n",
    "        self.conv1_2 = nn.Conv1d(256, 128, 3, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "        \n",
    "        \"\"\"Image Features\"\"\"\n",
    "        self.conv3_1 = nn.Conv3d(1, 64, (5,7,7), (2,2,2), padding=(2,3,3), dilation=1, groups=1, bias=True)\n",
    "        self.pool3_1 = nn.MaxPool3d((1,3,3), (1,2,2), padding=(0,1,1))\n",
    "        self.im_net_1 = self._make_layer(Block3, 64, 64, (3,3,3), (2,2,2), 2)\n",
    "\n",
    "        \"\"\"Fuse Features\"\"\"\n",
    "        self.fractional_maxpool = nn.FractionalMaxPool2d((3,1), output_size=(10, 1))\n",
    "        self.conv3_2 = nn.Conv3d(192, 512, (1, 1, 1))\n",
    "        self.conv3_3 = nn.Conv3d(512, 128, (1, 1, 1))\n",
    "        self.joint_net_1 = self._make_layer(Block3, 128, 128, (3,3,3), (2,2,2), 2)\n",
    "        self.joint_net_2 = self._make_layer(Block3, 128, 256, (3,3,3), (1,2,2), 2)\n",
    "        self.joint_net_3 = self._make_layer(Block3, 256, 512, (3,3,3), (1,2,2), 2)\n",
    "\n",
    "        #TODO: Global avg pooling, fc and sigmoid\n",
    "        self.fc = Linear(512,2)\n",
    "\n",
    "    def _make_layer(self, block, in_channels, out_channels, kernel_size, stride, blocks):\n",
    "        downsample = None\n",
    "        if stride != 1 or in_channels != out_channels * block.expansion:\n",
    "            if isinstance(kernel_size, int):\n",
    "                downsample = nn.Sequential(\n",
    "                    nn.Conv1d(in_channels, out_channels * block.expansion, kernel_size, stride),\n",
    "                    nn.BatchNorm1d(out_channels * block.expansion),\n",
    "                )\n",
    "                layers = []\n",
    "                layers.append(block(in_channels, out_channels, kernel_size, stride, downsample))\n",
    "            else:\n",
    "                downsample = nn.Sequential(\n",
    "                    nn.Conv3d(in_channels, out_channels * block.expansion, kernel_size, stride, padding=1),\n",
    "                    nn.BatchNorm3d(out_channels * block.expansion),\n",
    "                )\n",
    "                layers = []\n",
    "                layers.append(block(in_channels, out_channels, kernel_size, stride, downsample, padding=1))\n",
    "\n",
    "        \n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, sounds, images):\n",
    "        batchsize = sounds.shape[0]\n",
    "        sounds = sounds.view(batchsize, 2, -1)\n",
    "        _, num, _, xd, yd, = images.shape\n",
    "        images = images.view(batchsize, 1, num, xd, yd)\n",
    "        \n",
    "        out_s = self.conv1_1(sounds)\n",
    "        out_s = self.pool1_1(out_s)\n",
    "\n",
    "        out_s = self.s_net_1(out_s)\n",
    "        out_s = self.s_net_2(out_s)\n",
    "        out_s = self.s_net_3(out_s)\n",
    "\n",
    "        out_s = self.pool1_2(out_s)\n",
    "        out_s = self.conv1_2(out_s)\n",
    "        \n",
    "        out_im = self.conv3_1(images)\n",
    "        out_im = self.pool3_1(out_im)\n",
    "        out_im = self.im_net_1(out_im)\n",
    "\n",
    "        #tile audio, concatenate channel wise\n",
    "        out_s = self.fractional_maxpool(out_s.unsqueeze(3)) # Reduce dimension from 25 to 8\n",
    "        out_s = out_s.squeeze(3).view(-1, 1, 1).repeat(1, 28, 28).view(-1,128,10,28,28) # Tile\n",
    "        out_joint = torch.cat((out_s, out_im),1)\n",
    "        out_joint = self.conv3_2(out_joint)\n",
    "        out_joint = self.conv3_3(out_joint)\n",
    "        out_joint = self.joint_net_1(out_joint)\n",
    "        out_joint = self.joint_net_2(out_joint)\n",
    "        out_joint = self.joint_net_3(out_joint)\n",
    "        feature_maps = out_joint\n",
    "        \"\"\"Global Average Pooling\"\"\"\n",
    "        out_joint = F.avg_pool3d(out_joint, kernel_size=out_joint.size()[2:]).view(batchsize,-1)\n",
    "#         out_joint = out_joint.view(batchsize, 512, -1).mean(2)\n",
    "        out_joint = self.fc(out_joint)\n",
    "        out_joint = torch.sigmoid(out_joint)\n",
    "        return out_joint, feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# train_dataset = AudioDataset(train=True,transform=transform,h5_file='/media/jeff/Backup/CS598PS/data_nice_2597.h5')\n",
    "test_dataset = AudioDataset(train=False,transform=transform,h5_file='/media/jeff/Backup/CS598PS/data_nice_2597.h5')\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "807"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model\n",
    "model_align = alignment().cuda()\n",
    "checkpoint = torch.load(\"models/silas_model.pth\")\n",
    "model_align.load_state_dict(checkpoint.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Validation :', 0.75242279578909699, 0.49566294919454773)\n",
      "('Validation :', 0.74918198762773136, 0.49442379182156132)\n",
      "('Validation :', 0.76109657367365957, 0.48327137546468402)\n",
      "('Validation :', 0.74426339025686516, 0.50681536555142503)\n",
      "('Validation :', 0.75176194192013035, 0.5018587360594795)\n",
      "('Validation :', 0.76335253680062565, 0.50557620817843862)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-9:\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torch/multiprocessing/queue.py\", line 17, in send\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.py\", line 110, in _worker_loop\n",
      "    data_queue.put((idx, samples))\n",
      "  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 390, in put\n",
      "    return send(obj)\n",
      "    ForkingPickler(buf, pickle.HIGHEST_PROTOCOL).dump(obj)\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 224, in dump\n",
      "    self.save(obj)\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 554, in save_tuple\n",
      "    save(element)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c09ae118be2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0maligned_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_align\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msounds_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maligned_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0maccs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maligned_res\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 606, in save_list\n",
      "    self._batch_appends(iter(obj))\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 639, in _batch_appends\n",
      "    save(x)\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\n",
      "  File \"/usr/lib/python2.7/multiprocessing/forking.py\", line 67, in dispatcher\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 401, in save_reduce\n",
      "    save(args)\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 554, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python2.7/multiprocessing/forking.py\", line 66, in dispatcher\n",
      "    rv = reduce(obj)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torch/multiprocessing/reductions.py\", line 190, in reduce_storage\n",
      "    fd, size = storage._share_fd_()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('data/process/')\n",
    "\n",
    "cg = category_getter.CategoryGetter(\"data/process/eval_segments.csv\")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model_align.eval()\n",
    "labels_lst = ['Animal','Channel, environment and background','Human sounds','Music',\n",
    "             'Natural sounds','Sounds of things','Source-ambiguous sounds']\n",
    "accs_dict = {}\n",
    "for label in labels_lst:\n",
    "    accs_dict[label] = []\n",
    "\n",
    "filenames = np.load('/media/jeff/Backup/CS598PS/filenames_nice_test.npy')\n",
    "    \n",
    "for i in range(20):\n",
    "    accs = []\n",
    "    losses = []\n",
    "    for batch_idx, (images, sounds, labels) in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            images_v = Variable(images.type(torch.FloatTensor)).cuda()\n",
    "            sounds_v = Variable(sounds.type(torch.FloatTensor)).cuda()\n",
    "            labels_v = Variable(labels).cuda()\n",
    "            aligned_res, _ = model_align(sounds_v, images_v)\n",
    "            loss = loss_fn(aligned_res, labels_v)\n",
    "            losses.append(loss.item())\n",
    "            accs.append(np.mean((torch.argmax(aligned_res,1) == labels_v).detach().cpu().numpy()))\n",
    "\n",
    "            filename = str(filenames[batch_idx])\n",
    "            youtube_id = '_'.join(filename.split(\"_\")[:-2])\n",
    "            for id in cg.get_general_categories_for_video(youtube_id):\n",
    "                accs_dict[cg.ontology.get_record_for_id(id)[\"name\"]].append(np.mean((torch.argmax(aligned_res,1) == labels_v).detach().cpu().numpy()))\n",
    "    print(\"Validation :\", np.mean(losses), np.mean(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Animal', 25, 0.56000000000000005)\n",
      "('Channel, environment and background', 342, 0.50877192982456143)\n",
      "('Human sounds', 778, 0.53084832904884316)\n",
      "('Music', 4153, 0.49554538887551169)\n",
      "('Natural sounds', 0, nan)\n",
      "('Sounds of things', 1628, 0.50368550368550369)\n",
      "('Source-ambiguous sounds', 167, 0.47904191616766467)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python2.7/dist-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "for label in labels_lst:\n",
    "    print(label, len(accs_dict[label]), np.mean(accs_dict[label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Category splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg = category_getter.CategoryGetter(\"unbalanced_train_segments.csv\")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model_align.eval()\n",
    "labels_lst = ['Animal','Channel, environment and background','Human sounds','Music',\n",
    "             'Natural sounds','Sounds of things','Source-ambiguous sounds']\n",
    "accs_dict = {}\n",
    "for label in labels_lst:\n",
    "    accs_dict[label] = []\n",
    "\n",
    "    filenames = np.load('/media/jeff/Backup/CS598PS/data_nice_filenames.npy')\n",
    "\n",
    "for i in range(5):\n",
    "    accs = []\n",
    "    losses = []\n",
    "    for batch_idx, (images, sounds, labels) in enumerate(train_loader):\n",
    "        with torch.no_grad():\n",
    "            images_v = Variable(images.type(torch.FloatTensor)).cuda()\n",
    "            sounds_v = Variable(sounds.type(torch.FloatTensor)).cuda()\n",
    "            labels_v = Variable(labels).cuda()\n",
    "            aligned_res, _ = model_align(sounds_v, images_v)\n",
    "            loss = loss_fn(aligned_res, labels_v)\n",
    "            losses.append(loss.item())\n",
    "            accs.append(np.mean((torch.argmax(aligned_res,1) == labels_v).detach().cpu().numpy()))\n",
    "\n",
    "            filename = str(filenames[batch_idx])\n",
    "            youtube_id = '_'.join(filename.split(\"_\")[:-2])\n",
    "            for id in cg.get_general_categories_for_video(youtube_id):\n",
    "                accs_dict[cg.ontology.get_record_for_id(id)[\"name\"]].append(np.mean((torch.argmax(aligned_res,1) == labels_v).detach().cpu().numpy()))\n",
    "    print(\"Validation :\", np.mean(losses), np.mean(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in labels_lst:\n",
    "    print(label, len(accs_dict[label]), np.mean(accs_dict[label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
