{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py  \n",
    "import numpy as np\n",
    "import os, sys\n",
    "from scipy.misc import imresize\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "sys.path.append('data/process/')\n",
    "import category_getter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, train, frames_len=40, transform=None, h5_file='/media/jeff/Backup/CS598PS/data_2682.h5', transform_label=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            train (bool): Whether or not to use training data\n",
    "            frames (int): Number of video frames per video sample\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.frames_len = frames_len\n",
    "        \n",
    "        if self.train:\n",
    "            pass\n",
    "#             dataset = h5py.File(h5_file,'r')\n",
    "#             self.videos_train = np.array(dataset['videos_train'])\n",
    "#             self.sounds_train = np.array(dataset['sounds_train'])\n",
    "        else:\n",
    "            dataset = h5py.File(h5_file,'r')\n",
    "            self.videos_test = np.array(dataset['videos_test'])\n",
    "            self.sounds_test = np.array(dataset['sounds_test'])\n",
    "#             self.filenames_test = np.load('/media/jeff/Backup/CS598PS/test_filenames.npy')\n",
    "        dataset.close()\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.videos_train)\n",
    "        return len(self.videos_test)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.train:\n",
    "            image = self.videos_train[idx]\n",
    "            audio = self.sounds_train[idx]\n",
    "        else:\n",
    "            image = self.videos_test[idx]\n",
    "            audio = self.sounds_test[idx]\n",
    "#             filename = self.filenames_test[idx]\n",
    "\n",
    "        # Randomly sample 4 seconds from 10 second clip\n",
    "        if random.random() < 0.5:\n",
    "            start = random.randint(0,10) # Start frame\n",
    "        else:\n",
    "            start = random.randint(50,60)\n",
    "        new_image = np.zeros((self.frames_len,256,256,1), dtype=np.uint8)\n",
    "        for i in range(self.frames_len):\n",
    "            new_image[i] = np.expand_dims(image[start+i],2)\n",
    "        \n",
    "        # Randomly align or misalign audio sample\n",
    "        if random.random() < 0.5: # align\n",
    "            audio = audio[int(start*220500/100.0):int(start*220500/100.0)+88200]\n",
    "            label = 0\n",
    "        else: # misalign\n",
    "            if start < 30: # Add shift\n",
    "                shift = random.randint(20, 60-start) # frame shift amount\n",
    "                start = start+shift\n",
    "            else: # Subtract shift\n",
    "                shift = random.randint(20, start) # frame shift amount\n",
    "                start = start-shift\n",
    "            audio = audio[int(start*220500/100.0):int(start*220500/100.0)+88200]\n",
    "            label = 1\n",
    "            \n",
    "        transform_image = np.zeros((self.frames_len,1,224,224))\n",
    "        if self.transform:\n",
    "            for i in range(self.frames_len):\n",
    "                transform_image[i] = self.transform(new_image[i]) # Transform image frames\n",
    "        \n",
    "        return (transform_image, audio, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Block2(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, downsample=None):\n",
    "        super(Block2, self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding=0, dilation=1, groups=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=1, stride=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Block3(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=(1,1,1), stride=1, downsample=None, padding=0):\n",
    "        super(Block3, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding=padding, dilation=1, groups=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=(1,1,1), stride=1)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "def Linear(in_features, out_features, dropout=0.):\n",
    "    m = nn.Linear(in_features, out_features)\n",
    "    m.weight.data.normal_(mean=0, std=math.sqrt((1 - dropout) / in_features))\n",
    "    m.bias.data.zero_()\n",
    "    return nn.utils.weight_norm(m)\n",
    "\n",
    "class alignment(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(alignment, self).__init__()\n",
    "        \"\"\"Sound Features\"\"\"\n",
    "        self.conv1_1 = nn.Conv1d(2, 64, 65, stride=4, padding=0, dilation=1, groups=1, bias=True)\n",
    "        self.pool1_1 = nn.MaxPool1d(4, stride=4)\n",
    "\n",
    "        self.s_net_1 = self._make_layer(Block2, 64, 128, 15, 4, 1)\n",
    "        self.s_net_2 = self._make_layer(Block2, 128, 128, 15, 4, 1)\n",
    "        self.s_net_3 = self._make_layer(Block2, 128, 256, 15, 4, 1)\n",
    "        \n",
    "        self.pool1_2 = nn.MaxPool1d(3, stride=3)\n",
    "        self.conv1_2 = nn.Conv1d(256, 128, 3, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "        \n",
    "        \"\"\"Image Features\"\"\"\n",
    "        self.conv3_1 = nn.Conv3d(1, 64, (5,7,7), (2,2,2), padding=(2,3,3), dilation=1, groups=1, bias=True)\n",
    "        self.pool3_1 = nn.MaxPool3d((1,3,3), (1,2,2), padding=(0,1,1))\n",
    "        self.im_net_1 = self._make_layer(Block3, 64, 64, (3,3,3), (2,2,2), 2)\n",
    "\n",
    "        \"\"\"Fuse Features\"\"\"\n",
    "        self.fractional_maxpool = nn.FractionalMaxPool2d((3,1), output_size=(10, 1))\n",
    "        self.conv3_2 = nn.Conv3d(192, 512, (1, 1, 1))\n",
    "        self.conv3_3 = nn.Conv3d(512, 128, (1, 1, 1))\n",
    "        self.joint_net_1 = self._make_layer(Block3, 128, 128, (3,3,3), (2,2,2), 2)\n",
    "        self.joint_net_2 = self._make_layer(Block3, 128, 256, (3,3,3), (1,2,2), 2)\n",
    "        self.joint_net_3 = self._make_layer(Block3, 256, 512, (3,3,3), (1,2,2), 2)\n",
    "\n",
    "        #TODO: Global avg pooling, fc and sigmoid\n",
    "        self.fc = Linear(512,2)\n",
    "\n",
    "    def _make_layer(self, block, in_channels, out_channels, kernel_size, stride, blocks):\n",
    "        downsample = None\n",
    "        if stride != 1 or in_channels != out_channels * block.expansion:\n",
    "            if isinstance(kernel_size, int):\n",
    "                downsample = nn.Sequential(\n",
    "                    nn.Conv1d(in_channels, out_channels * block.expansion, kernel_size, stride),\n",
    "                    nn.BatchNorm1d(out_channels * block.expansion),\n",
    "                )\n",
    "                layers = []\n",
    "                layers.append(block(in_channels, out_channels, kernel_size, stride, downsample))\n",
    "            else:\n",
    "                downsample = nn.Sequential(\n",
    "                    nn.Conv3d(in_channels, out_channels * block.expansion, kernel_size, stride, padding=1),\n",
    "                    nn.BatchNorm3d(out_channels * block.expansion),\n",
    "                )\n",
    "                layers = []\n",
    "                layers.append(block(in_channels, out_channels, kernel_size, stride, downsample, padding=1))\n",
    "\n",
    "        \n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, sounds, images):\n",
    "        batchsize = sounds.shape[0]\n",
    "        sounds = sounds.view(batchsize, 2, -1)\n",
    "        _, num, _, xd, yd, = images.shape\n",
    "        images = images.view(batchsize, 1, num, xd, yd)\n",
    "        \n",
    "        out_s = self.conv1_1(sounds)\n",
    "        out_s = self.pool1_1(out_s)\n",
    "\n",
    "        out_s = self.s_net_1(out_s)\n",
    "        out_s = self.s_net_2(out_s)\n",
    "        out_s = self.s_net_3(out_s)\n",
    "\n",
    "        out_s = self.pool1_2(out_s)\n",
    "        out_s = self.conv1_2(out_s)\n",
    "        \n",
    "        out_im = self.conv3_1(images)\n",
    "        out_im = self.pool3_1(out_im)\n",
    "        out_im = self.im_net_1(out_im)\n",
    "\n",
    "        #tile audio, concatenate channel wise\n",
    "        out_s = self.fractional_maxpool(out_s.unsqueeze(3)) # Reduce dimension from 25 to 8\n",
    "        out_s = out_s.squeeze(3).view(-1, 1, 1).repeat(1, 28, 28).view(-1,128,10,28,28) # Tile\n",
    "        out_joint = torch.cat((out_s, out_im),1)\n",
    "        out_joint = self.conv3_2(out_joint)\n",
    "        out_joint = self.conv3_3(out_joint)\n",
    "        out_joint = self.joint_net_1(out_joint)\n",
    "        out_joint = self.joint_net_2(out_joint)\n",
    "        out_joint = self.joint_net_3(out_joint)\n",
    "        feature_maps = out_joint\n",
    "        \"\"\"Global Average Pooling\"\"\"\n",
    "        out_joint = F.avg_pool3d(out_joint, kernel_size=out_joint.size()[2:]).view(batchsize,-1)\n",
    "#         out_joint = out_joint.view(batchsize, 512, -1).mean(2)\n",
    "        out_joint = self.fc(out_joint)\n",
    "        out_joint = torch.sigmoid(out_joint)\n",
    "        return out_joint, feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# train_dataset = AudioDataset(train=True,transform=transform,h5_file='/media/jeff/Backup/CS598PS/data_nice_2597.h5')\n",
    "test_dataset = AudioDataset(train=False,transform=transform,h5_file='/data/jz/CS598PS/data/data_nice_2597.h5')\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "807"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import pickle\n",
    "pickle.load = partial(pickle.load, encoding=\"latin1\")\n",
    "pickle.Unpickler = partial(pickle.Unpickler, encoding=\"latin1\")\n",
    "model_align = torch.load(\"nice_lr6_b8.pth\", map_location=lambda storage, loc: storage, pickle_module=pickle).cuda()\n",
    "# torch.load(\"nice_lr4_b16.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model import alignment\n",
    "# model_align = alignment().cuda()\n",
    "# checkpoint = torch.load(\"nice_lr6_b8_faster.pth\")\n",
    "# model_align.load_state_dict(checkpoint.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation : 0.712316347763884 0.4882280049566295\n",
      "Validation : 0.6951811615714649 0.5080545229244114\n",
      "Validation : 0.7053294151541321 0.5142503097893433\n",
      "Validation : 0.7067492906341175 0.5092936802973977\n",
      "Validation : 0.7076373519802921 0.5018587360594795\n",
      "Validation : 0.7032886238346313 0.5130111524163569\n",
      "Validation : 0.7070514259876076 0.4993804213135068\n",
      "Validation : 0.6990849081763844 0.5241635687732342\n",
      "Validation : 0.7035892461016866 0.5092936802973977\n",
      "Validation : 0.7159336813468177 0.4795539033457249\n",
      "Validation : 0.7014951581848599 0.5241635687732342\n",
      "Validation : 0.7053794619319164 0.5092936802973977\n",
      "Validation : 0.7272843711881389 0.45724907063197023\n",
      "Validation : 0.7067673418306212 0.4708798017348203\n",
      "Validation : 0.7043960706008855 0.5179677819083024\n",
      "Validation : 0.7032131319300098 0.5043370508054523\n",
      "Validation : 0.7051349273401565 0.49318463444857497\n",
      "Validation : 0.7103072861313376 0.4770755885997522\n",
      "Validation : 0.704893493297818 0.5055762081784386\n",
      "Validation : 0.7195374128100597 0.4547707558859975\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('data/process/')\n",
    "\n",
    "cg = category_getter.CategoryGetter(\"data/process/eval_segments.csv\")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model_align.eval()\n",
    "labels_lst = ['Animal','Channel, environment and background','Human sounds','Music',\n",
    "             'Natural sounds','Sounds of things','Source-ambiguous sounds']\n",
    "accs_dict = {}\n",
    "for label in labels_lst:\n",
    "    accs_dict[label] = []\n",
    "\n",
    "filenames = np.load('/data/jz/CS598PS/data/filenames_nice_test.npy')\n",
    "    \n",
    "accs_lst = []\n",
    "losses_lst = []\n",
    "for i in range(20):\n",
    "    accs = []\n",
    "    losses = []\n",
    "    for batch_idx, (images, sounds, labels) in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            images_v = Variable(images.type(torch.FloatTensor)).cuda()\n",
    "            sounds_v = Variable(sounds.type(torch.FloatTensor)).cuda()\n",
    "            labels_v = Variable(labels).cuda()\n",
    "            aligned_res, _ = model_align(sounds_v, images_v)\n",
    "            loss = loss_fn(aligned_res, labels_v)\n",
    "            losses.append(loss.item())\n",
    "            accs.append(np.mean((torch.argmax(aligned_res,1) == labels_v).detach().cpu().numpy()))\n",
    "\n",
    "            filename = str(filenames[batch_idx])\n",
    "            youtube_id = '_'.join(filename.split(\"_\")[:-2])\n",
    "            for id in cg.get_general_categories_for_video(youtube_id):\n",
    "                accs_dict[cg.ontology.get_record_for_id(id)[\"name\"]].append(np.mean((torch.argmax(aligned_res,1) == labels_v).detach().cpu().numpy()))\n",
    "    accs_lst.append(np.mean(accs))\n",
    "    losses_lst.append(np.mean(losses))\n",
    "    print(\"Validation :\", np.mean(losses), np.mean(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animal 80 0.55\n",
      "Channel, environment and background 1020 0.47843137254901963\n",
      "Human sounds 2360 0.4966101694915254\n",
      "Music 12740 0.49623233908948194\n",
      "Natural sounds 0 nan\n",
      "Sounds of things 4880 0.49938524590163935\n",
      "Source-ambiguous sounds 520 0.4846153846153846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nfs/jz41/.local/lib/python3.5/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/nfs/jz41/.local/lib/python3.5/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "for label in labels_lst:\n",
    "    print(label, len(accs_dict[label]), np.mean(accs_dict[label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Category splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg = category_getter.CategoryGetter(\"unbalanced_train_segments.csv\")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model_align.eval()\n",
    "labels_lst = ['Animal','Channel, environment and background','Human sounds','Music',\n",
    "             'Natural sounds','Sounds of things','Source-ambiguous sounds']\n",
    "accs_dict = {}\n",
    "for label in labels_lst:\n",
    "    accs_dict[label] = []\n",
    "\n",
    "    filenames = np.load('/media/jeff/Backup/CS598PS/data_nice_filenames.npy')\n",
    "\n",
    "for i in range(5):\n",
    "    accs = []\n",
    "    losses = []\n",
    "    for batch_idx, (images, sounds, labels) in enumerate(train_loader):\n",
    "        with torch.no_grad():\n",
    "            images_v = Variable(images.type(torch.FloatTensor)).cuda()\n",
    "            sounds_v = Variable(sounds.type(torch.FloatTensor)).cuda()\n",
    "            labels_v = Variable(labels).cuda()\n",
    "            aligned_res, _ = model_align(sounds_v, images_v)\n",
    "            loss = loss_fn(aligned_res, labels_v)\n",
    "            losses.append(loss.item())\n",
    "            accs.append(np.mean((torch.argmax(aligned_res,1) == labels_v).detach().cpu().numpy()))\n",
    "\n",
    "            filename = str(filenames[batch_idx])\n",
    "            youtube_id = '_'.join(filename.split(\"_\")[:-2])\n",
    "            for id in cg.get_general_categories_for_video(youtube_id):\n",
    "                accs_dict[cg.ontology.get_record_for_id(id)[\"name\"]].append(np.mean((torch.argmax(aligned_res,1) == labels_v).detach().cpu().numpy()))\n",
    "    print(\"Validation :\", np.mean(losses), np.mean(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in labels_lst:\n",
    "    print(label, len(accs_dict[label]), np.mean(accs_dict[label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
