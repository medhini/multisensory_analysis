{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py  \n",
    "import numpy as np\n",
    "import os \n",
    "from scipy.misc import imresize\n",
    "import cv2\n",
    "import random\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, train, frames_len=40, transform=None, h5_file='data/data.h5', transform_label=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            train (bool): Whether or not to use training data\n",
    "            frames (int): Number of video frames per video sample\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.frames_len = frames_len\n",
    "        \n",
    "        dataset = h5py.File(h5_file)\n",
    "        self.videos_train = np.array(dataset['videos_train'])\n",
    "        self.sounds_train = np.array(dataset['sounds_train'])\n",
    "        self.videos_test = np.array(dataset['videos_test'])\n",
    "        self.sounds_test = np.array(dataset['sounds_test'])\n",
    "        dataset.close()\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.videos_train)\n",
    "        return len(self.videos_test)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.train:\n",
    "            image = self.videos_train[idx]\n",
    "            audio = self.sounds_train[idx]\n",
    "        else:\n",
    "            image = self.videos_test[idx]\n",
    "            audio = self.sounds_test[idx]\n",
    "\n",
    "        # Randomly sample 4 seconds from 10 second clip\n",
    "        start = random.randint(0, 100-self.frames_len) # Start frame\n",
    "        new_image = np.zeros((self.frames_len,256,256,1), dtype=np.uint8)\n",
    "        for i in range(self.frames_len):\n",
    "            new_image[i] = np.expand_dims(image[start+i],2)\n",
    "        \n",
    "        # Randomly align or misalign audio sample\n",
    "        if random.random() < 0.5: # align\n",
    "            audio = audio[int(start*220500/100.0):int(start*220500/100.0)+88200]\n",
    "            label = 0\n",
    "        else: # misalign\n",
    "            shift = random.randint(20, 60) # frame shift amount\n",
    "            if random.random() < 0.5: # Add shift\n",
    "                start = np.clip(start-shift, 0, 100-self.frames_len)\n",
    "            else: # Subtract shift\n",
    "                start = np.clip(start+shift, 0, 100-self.frames_len)\n",
    "            audio = audio[int(start*220500/100.0):int(start*220500/100.0)+88200]\n",
    "            label = 1\n",
    "            \n",
    "        transform_image = np.zeros((self.frames_len,1,224,224), dtype=np.uint8)\n",
    "        if self.transform:\n",
    "            for i in range(self.frames_len):\n",
    "                transform_image[i] = self.transform(new_image[i]) # Transform image frames\n",
    "            \n",
    "        return (transform_image, audio, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Block2(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, downsample=None):\n",
    "        super(Block2, self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding=0, dilation=1, groups=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=1, stride=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Block3(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=(1,1,1), stride=1, downsample=None, padding=0):\n",
    "        super(Block3, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding=padding, dilation=1, groups=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=(1,1,1), stride=1)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "def Linear(in_features, out_features, dropout=0.):\n",
    "    m = nn.Linear(in_features, out_features)\n",
    "    m.weight.data.normal_(mean=0, std=math.sqrt((1 - dropout) / in_features))\n",
    "    m.bias.data.zero_()\n",
    "    return nn.utils.weight_norm(m)\n",
    "\n",
    "class alignment(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(alignment, self).__init__()\n",
    "        \"\"\"Sound Features\"\"\"\n",
    "        self.conv1_1 = nn.Conv1d(2, 64, 65, stride=4, padding=0, dilation=1, groups=1, bias=True)\n",
    "        self.pool1_1 = nn.MaxPool1d(4, stride=4)\n",
    "\n",
    "        self.s_net_1 = self._make_layer(Block2, 64, 128, 15, 4, 1)\n",
    "        self.s_net_2 = self._make_layer(Block2, 128, 128, 15, 4, 1)\n",
    "        self.s_net_3 = self._make_layer(Block2, 128, 256, 15, 4, 1)\n",
    "        \n",
    "        self.pool1_2 = nn.MaxPool1d(3, stride=3)\n",
    "        self.conv1_2 = nn.Conv1d(256, 128, 3, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "        \n",
    "        \"\"\"Image Features\"\"\"\n",
    "        self.conv3_1 = nn.Conv3d(1, 64, (5,7,7), (2,2,2), padding=(2,3,3), dilation=1, groups=1, bias=True)\n",
    "        self.pool3_1 = nn.MaxPool3d((1,3,3), (1,2,2), padding=(0,1,1))\n",
    "        self.im_net_1 = self._make_layer(Block3, 64, 64, (3,3,3), (2,2,2), 2)\n",
    "\n",
    "        \"\"\"Fuse Features\"\"\"\n",
    "        self.fractional_maxpool = nn.FractionalMaxPool2d((3,1), output_size=(10, 1))\n",
    "        self.conv3_2 = nn.Conv3d(192, 512, (1, 1, 1))\n",
    "        self.conv3_3 = nn.Conv3d(512, 128, (1, 1, 1))\n",
    "        self.joint_net_1 = self._make_layer(Block3, 128, 128, (3,3,3), (2,2,2), 2)\n",
    "        self.joint_net_2 = self._make_layer(Block3, 128, 256, (3,3,3), (1,2,2), 2)\n",
    "        self.joint_net_3 = self._make_layer(Block3, 256, 512, (3,3,3), (1,2,2), 2)\n",
    "\n",
    "        #TODO: Global avg pooling, fc and sigmoid\n",
    "        self.fc = Linear(512,512)\n",
    "\n",
    "    def _make_layer(self, block, in_channels, out_channels, kernel_size, stride, blocks):\n",
    "        downsample = None\n",
    "        if stride != 1 or in_channels != out_channels * block.expansion:\n",
    "            if isinstance(kernel_size, int):\n",
    "                downsample = nn.Sequential(\n",
    "                    nn.Conv1d(in_channels, out_channels * block.expansion, kernel_size, stride),\n",
    "                    nn.BatchNorm1d(out_channels * block.expansion),\n",
    "                )\n",
    "                layers = []\n",
    "                layers.append(block(in_channels, out_channels, kernel_size, stride, downsample))\n",
    "            else:\n",
    "                downsample = nn.Sequential(\n",
    "                    nn.Conv3d(in_channels, out_channels * block.expansion, kernel_size, stride, padding=1),\n",
    "                    nn.BatchNorm3d(out_channels * block.expansion),\n",
    "                )\n",
    "                layers = []\n",
    "                layers.append(block(in_channels, out_channels, kernel_size, stride, downsample, padding=1))\n",
    "\n",
    "        \n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    \n",
    "\n",
    "    # def block_2(self, in_channels, out_channels, kernel_size, stride, downsample=None):\n",
    "    #     self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding=0, dilation=1, groups=1, bias=True)\n",
    "    #     self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "    #     self.relu = nn.ReLU(inplace=True)\n",
    "    #     self.conv2 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding=0, dilation=1, groups=1, bias=True)\n",
    "    #     self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "    #     self.downsample = downsample\n",
    "    #     self.stride = stride\n",
    "\n",
    "    # def block_3(self, in_channels, out_channels, kernel_size, stride, downsample=None):\n",
    "    #     self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding=0, dilation=1, groups=1, bias=True)\n",
    "    #     self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "    #     self.relu = nn.ReLU(inplace=True)\n",
    "    #     self.conv2 = nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding=0, dilation=1, groups=1, bias=True)\n",
    "    #     self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "    #     self.downsample = downsample\n",
    "    #     self.stride = stride\n",
    "\n",
    "    def forward(self, batchsize, sounds, images):\n",
    "        sounds = sounds.view(batchsize, 2, -1)\n",
    "        _, num, _, xd, yd, = images.shape\n",
    "        images = images.view(batchsize, 1, num, xd, yd)\n",
    "        \n",
    "        out_s = self.conv1_1(sounds)\n",
    "        out_s = self.pool1_1(out_s)\n",
    "\n",
    "        out_s = self.s_net_1(out_s)\n",
    "        out_s = self.s_net_2(out_s)\n",
    "        out_s = self.s_net_3(out_s)\n",
    "\n",
    "        out_s = self.pool1_2(out_s)\n",
    "        out_s = self.conv1_2(out_s)\n",
    "        \n",
    "        out_im = self.conv3_1(images)\n",
    "        out_im = self.pool3_1(out_im)\n",
    "        out_im = self.im_net_1(out_im)\n",
    "\n",
    "        #tile audio, concatenate channel wise\n",
    "        out_s = self.fractional_maxpool(out_s.unsqueeze(3)) # Reduce dimension from 25 to 8\n",
    "        out_s = out_s.squeeze(3).view(-1, 1, 1).repeat(1, 28, 28).view(-1,128,10,28,28) # Tile\n",
    "        out_joint = torch.cat((out_s, out_im),1)\n",
    "        out_joint = self.conv3_2(out_joint)\n",
    "        out_joint = self.conv3_3(out_joint)\n",
    "        out_joint = self.joint_net_1(out_joint)\n",
    "        out_joint = self.joint_net_2(out_joint)\n",
    "        out_joint = self.joint_net_3(out_joint)\n",
    "        print(out_joint.shape)\n",
    "        out_joint = self.fc(out_joint)\n",
    "        out_joint = F.sigmoid(out_joint)\n",
    "        \n",
    "        return out_joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "transforms.ToPILImage(),\n",
    "transforms.RandomHorizontalFlip(),\n",
    "transforms.RandomCrop(224),\n",
    "transforms.ToTensor()])\n",
    "\n",
    "train_dataset = AudioDataset(train=True,transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=5, shuffle=True, num_workers=4)\n",
    "\n",
    "model_align = alignment()\n",
    "model_align.cuda()\n",
    "model_align.train(True)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_align = optim.Adam(model_align.parameters(), lr = 1e-4)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for batch_idx, (images, sounds, labels) in enumerate(train_loader):\n",
    "        images_v = Variable(images.type(torch.FloatTensor)).cuda()\n",
    "        sounds_v = Variable(sounds.type(torch.FloatTensor)).cuda()\n",
    "\n",
    "        optimizer_align.zero_grad()\n",
    "\n",
    "        aligned_res = model_align(5, sounds_v, images_v)\n",
    "\n",
    "        loss = loss_fn(aligned_res, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer_align.step()\n",
    "\n",
    "        print(images.shape)\n",
    "        print(sounds.shape)\n",
    "        print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
