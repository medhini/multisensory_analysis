{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py  \n",
    "import numpy as np\n",
    "import os \n",
    "from scipy.misc import imresize\n",
    "import cv2\n",
    "import random\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, train, frames_len=40, transform=None, h5_file='/media/jeff/Backup/CS598PS/data_2682.h5', transform_label=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            train (bool): Whether or not to use training data\n",
    "            frames (int): Number of video frames per video sample\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.frames_len = frames_len\n",
    "        \n",
    "        dataset = h5py.File(h5_file)\n",
    "        if self.train:\n",
    "            self.videos_train = np.array(dataset['videos_train'])\n",
    "            self.sounds_train = np.array(dataset['sounds_train'])\n",
    "        else:\n",
    "            self.videos_test = np.array(dataset['videos_test'])\n",
    "            self.sounds_test = np.array(dataset['sounds_test'])\n",
    "        dataset.close()\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.videos_train)\n",
    "        return len(self.videos_test)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.train:\n",
    "            image = self.videos_train[idx]\n",
    "            audio = self.sounds_train[idx]\n",
    "        else:\n",
    "            image = self.videos_test[idx]\n",
    "            audio = self.sounds_test[idx]\n",
    "\n",
    "        # Randomly sample 4 seconds from 10 second clip\n",
    "        if random.random() < 0.5:\n",
    "            start = random.randint(0,10) # Start frame\n",
    "        else:\n",
    "            start = random.randint(50,60)\n",
    "        new_image = np.zeros((self.frames_len,256,256,1), dtype=np.uint8)\n",
    "        for i in range(self.frames_len):\n",
    "            new_image[i] = np.expand_dims(image[start+i],2)\n",
    "        \n",
    "        # Randomly align or misalign audio sample\n",
    "        if random.random() < 0.5: # align\n",
    "            audio = audio[int(start*220500/100.0):int(start*220500/100.0)+88200]\n",
    "            label = 0\n",
    "        else: # misalign\n",
    "            if start < 30: # Add shift\n",
    "                shift = random.randint(20, 60-start) # frame shift amount\n",
    "#                 start = np.clip(start+shift, 0, 100-self.frames_len)\n",
    "                start = start+shift\n",
    "            else: # Subtract shift\n",
    "                shift = random.randint(20, start) # frame shift amount\n",
    "#                 start = np.clip(start-shift, 0, 100-self.frames_len)\n",
    "                start = start-shift\n",
    "            audio = audio[int(start*220500/100.0):int(start*220500/100.0)+88200]\n",
    "            label = 1\n",
    "            \n",
    "        transform_image = np.zeros((self.frames_len,1,224,224))\n",
    "        if self.transform:\n",
    "            for i in range(self.frames_len):\n",
    "                transform_image[i] = self.transform(new_image[i]) # Transform image frames\n",
    "        \n",
    "        return (transform_image, audio, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Block2(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, downsample=None):\n",
    "        super(Block2, self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding=0, dilation=1, groups=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=1, stride=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Block3(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=(1,1,1), stride=1, downsample=None, padding=0):\n",
    "        super(Block3, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding=padding, dilation=1, groups=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=(1,1,1), stride=1)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "def Linear(in_features, out_features, dropout=0.):\n",
    "    m = nn.Linear(in_features, out_features)\n",
    "    m.weight.data.normal_(mean=0, std=math.sqrt((1 - dropout) / in_features))\n",
    "    m.bias.data.zero_()\n",
    "    return nn.utils.weight_norm(m)\n",
    "\n",
    "class alignment(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(alignment, self).__init__()\n",
    "        \"\"\"Sound Features\"\"\"\n",
    "        self.conv1_1 = nn.Conv1d(2, 64, 65, stride=4, padding=0, dilation=1, groups=1, bias=True)\n",
    "        self.pool1_1 = nn.MaxPool1d(4, stride=4)\n",
    "\n",
    "        self.s_net_1 = self._make_layer(Block2, 64, 128, 15, 4, 1)\n",
    "        self.s_net_2 = self._make_layer(Block2, 128, 128, 15, 4, 1)\n",
    "        self.s_net_3 = self._make_layer(Block2, 128, 256, 15, 4, 1)\n",
    "        \n",
    "        self.pool1_2 = nn.MaxPool1d(3, stride=3)\n",
    "        self.conv1_2 = nn.Conv1d(256, 128, 3, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "        \n",
    "        \"\"\"Image Features\"\"\"\n",
    "        self.conv3_1 = nn.Conv3d(1, 64, (5,7,7), (2,2,2), padding=(2,3,3), dilation=1, groups=1, bias=True)\n",
    "        self.pool3_1 = nn.MaxPool3d((1,3,3), (1,2,2), padding=(0,1,1))\n",
    "        self.im_net_1 = self._make_layer(Block3, 64, 64, (3,3,3), (2,2,2), 2)\n",
    "\n",
    "        \"\"\"Fuse Features\"\"\"\n",
    "        self.fractional_maxpool = nn.FractionalMaxPool2d((3,1), output_size=(10, 1))\n",
    "        self.conv3_2 = nn.Conv3d(192, 512, (1, 1, 1))\n",
    "        self.conv3_3 = nn.Conv3d(512, 128, (1, 1, 1))\n",
    "        self.joint_net_1 = self._make_layer(Block3, 128, 128, (3,3,3), (2,2,2), 2)\n",
    "        self.joint_net_2 = self._make_layer(Block3, 128, 256, (3,3,3), (1,2,2), 2)\n",
    "        self.joint_net_3 = self._make_layer(Block3, 256, 512, (3,3,3), (1,2,2), 2)\n",
    "\n",
    "        #TODO: Global avg pooling, fc and sigmoid\n",
    "        self.fc = Linear(512,2)\n",
    "\n",
    "    def _make_layer(self, block, in_channels, out_channels, kernel_size, stride, blocks):\n",
    "        downsample = None\n",
    "        if stride != 1 or in_channels != out_channels * block.expansion:\n",
    "            if isinstance(kernel_size, int):\n",
    "                downsample = nn.Sequential(\n",
    "                    nn.Conv1d(in_channels, out_channels * block.expansion, kernel_size, stride),\n",
    "                    nn.BatchNorm1d(out_channels * block.expansion),\n",
    "                )\n",
    "                layers = []\n",
    "                layers.append(block(in_channels, out_channels, kernel_size, stride, downsample))\n",
    "            else:\n",
    "                downsample = nn.Sequential(\n",
    "                    nn.Conv3d(in_channels, out_channels * block.expansion, kernel_size, stride, padding=1),\n",
    "                    nn.BatchNorm3d(out_channels * block.expansion),\n",
    "                )\n",
    "                layers = []\n",
    "                layers.append(block(in_channels, out_channels, kernel_size, stride, downsample, padding=1))\n",
    "\n",
    "        \n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, sounds, images):\n",
    "        batchsize = sounds.shape[0]\n",
    "        sounds = sounds.view(batchsize, 2, -1)\n",
    "        _, num, _, xd, yd, = images.shape\n",
    "        images = images.view(batchsize, 1, num, xd, yd)\n",
    "        \n",
    "        out_s = self.conv1_1(sounds)\n",
    "        out_s = self.pool1_1(out_s)\n",
    "\n",
    "        out_s = self.s_net_1(out_s)\n",
    "        out_s = self.s_net_2(out_s)\n",
    "        out_s = self.s_net_3(out_s)\n",
    "\n",
    "        out_s = self.pool1_2(out_s)\n",
    "        out_s = self.conv1_2(out_s)\n",
    "        \n",
    "        out_im = self.conv3_1(images)\n",
    "        out_im = self.pool3_1(out_im)\n",
    "        out_im = self.im_net_1(out_im)\n",
    "\n",
    "        #tile audio, concatenate channel wise\n",
    "        out_s = self.fractional_maxpool(out_s.unsqueeze(3)) # Reduce dimension from 25 to 8\n",
    "        out_s = out_s.squeeze(3).view(-1, 1, 1).repeat(1, 28, 28).view(-1,128,10,28,28) # Tile\n",
    "        out_joint = torch.cat((out_s, out_im),1)\n",
    "        out_joint = self.conv3_2(out_joint)\n",
    "        out_joint = self.conv3_3(out_joint)\n",
    "        out_joint = self.joint_net_1(out_joint)\n",
    "        out_joint = self.joint_net_2(out_joint)\n",
    "        out_joint = self.joint_net_3(out_joint)\n",
    "        feature_maps = out_joint\n",
    "        \"\"\"Global Average Pooling\"\"\"\n",
    "        out_joint = F.avg_pool3d(out_joint, kernel_size=out_joint.size()[2:]).view(batchsize,-1)\n",
    "#         out_joint = out_joint.view(batchsize, 512, -1).mean(2)\n",
    "        out_joint = self.fc(out_joint)\n",
    "        out_joint = torch.sigmoid(out_joint)\n",
    "        return out_joint, feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "transforms.ToPILImage(),\n",
    "# transforms.RandomHorizontalFlip(),\n",
    "transforms.RandomCrop(224),\n",
    "transforms.ToTensor()])\n",
    "\n",
    "train_dataset = AudioDataset(train=True,transform=transform)\n",
    "test_dataset = AudioDataset(train=False,transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "\n",
    "model_align = alignment().cuda()\n",
    "checkpoint = torch.load(\"fixed_500.pth\")\n",
    "model_align.load_state_dict(checkpoint.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch :', 0, 0.7115155496904927, 0.5221774193548387)\n",
      "('Epoch :', 1, 0.7018449652579523, 0.48185483870967744)\n",
      "('Epoch :', 2, 0.6940048048573155, 0.49798387096774194)\n",
      "('Epoch :', 3, 0.6957579332013284, 0.4737903225806452)\n",
      "('Epoch :', 4, 0.6932148395046112, 0.5181451612903226)\n",
      "('Epoch :', 5, 0.6967367337596032, 0.5)\n",
      "('Epoch :', 6, 0.6955373383337452, 0.5141129032258065)\n",
      "('Epoch :', 7, 0.6927853841935435, 0.48185483870967744)\n",
      "('Epoch :', 8, 0.6944674080418002, 0.5020161290322581)\n",
      "('Epoch :', 9, 0.6935403904607219, 0.4879032258064516)\n",
      "('Epoch :', 10, 0.6943451569926354, 0.5120967741935484)\n",
      "('Epoch :', 11, 0.6919757877626727, 0.5342741935483871)\n",
      "('Epoch :', 12, 0.6982282303994701, 0.42338709677419356)\n",
      "('Epoch :', 13, 0.6933796328883017, 0.5020161290322581)\n",
      "('Epoch :', 14, 0.6932195559624703, 0.4838709677419355)\n",
      "('Epoch :', 15, 0.6938723575684332, 0.5342741935483871)\n",
      "('Epoch :', 16, 0.6937995591471272, 0.4899193548387097)\n",
      "('Epoch :', 17, 0.6955377920981376, 0.4637096774193548)\n",
      "('Epoch :', 18, 0.6958078742027283, 0.4879032258064516)\n",
      "('Epoch :', 19, 0.6948635462791689, 0.48588709677419356)\n",
      "('Epoch :', 20, 0.6946468680135666, 0.4838709677419355)\n",
      "('Epoch :', 21, 0.6928660984962217, 0.5443548387096774)\n",
      "('Epoch :', 22, 0.6948197830107904, 0.5)\n",
      "('Epoch :', 23, 0.6948180621670138, 0.4838709677419355)\n",
      "('Epoch :', 24, 0.6960817575454712, 0.48185483870967744)\n",
      "('Validation :', 24, 0.6943815625630892, 0.4935897435897436)\n",
      "('Epoch :', 25, 0.6921106423101118, 0.5241935483870968)\n",
      "('Epoch :', 26, 0.6883090388390326, 0.5)\n",
      "('Epoch :', 27, 0.694689191156818, 0.5)\n",
      "('Epoch :', 28, 0.6960283671655962, 0.4899193548387097)\n",
      "('Epoch :', 29, 0.6924344551178717, 0.5383064516129032)\n",
      "('Epoch :', 30, 0.6890416106870098, 0.5584677419354839)\n",
      "('Epoch :', 31, 0.6994384334933373, 0.5)\n",
      "('Epoch :', 32, 0.6934396143882505, 0.5201612903225806)\n",
      "('Epoch :', 33, 0.6961794649401019, 0.46774193548387094)\n",
      "('Epoch :', 34, 0.6931425659887253, 0.5)\n",
      "('Epoch :', 35, 0.6938535917189813, 0.47580645161290325)\n",
      "('Epoch :', 36, 0.6999392605596974, 0.4657258064516129)\n",
      "('Epoch :', 37, 0.694945712243357, 0.46169354838709675)\n",
      "('Epoch :', 38, 0.6935711522256175, 0.5241935483870968)\n",
      "('Epoch :', 39, 0.6908541110254103, 0.5060483870967742)\n",
      "('Epoch :', 40, 0.694856337962612, 0.5201612903225806)\n",
      "('Epoch :', 41, 0.6937599835857269, 0.5120967741935484)\n",
      "('Epoch :', 42, 0.6904895036451278, 0.5161290322580645)\n",
      "('Epoch :', 43, 0.6990892560251297, 0.4899193548387097)\n",
      "('Epoch :', 44, 0.6928951298036883, 0.49193548387096775)\n",
      "('Epoch :', 45, 0.6952877313860001, 0.4899193548387097)\n",
      "('Epoch :', 46, 0.6960516245134415, 0.4637096774193548)\n",
      "('Epoch :', 47, 0.6912553252712372, 0.5161290322580645)\n",
      "('Epoch :', 48, 0.6919764799456443, 0.5241935483870968)\n",
      "('Epoch :', 49, 0.6939242078411963, 0.5282258064516129)\n",
      "('Validation :', 49, 0.7017086285811204, 0.4262820512820513)\n",
      "('Epoch :', 50, 0.6951573183459621, 0.5020161290322581)\n",
      "('Epoch :', 51, 0.6916747035518769, 0.5020161290322581)\n",
      "('Epoch :', 52, 0.6956698048499322, 0.4879032258064516)\n",
      "('Epoch :', 53, 0.6944493324525894, 0.4737903225806452)\n",
      "('Epoch :', 54, 0.6936099798448624, 0.47580645161290325)\n",
      "('Epoch :', 55, 0.6932860939733444, 0.5020161290322581)\n",
      "('Epoch :', 56, 0.6957927038592677, 0.5161290322580645)\n",
      "('Epoch :', 57, 0.6927429495319244, 0.5040322580645161)\n",
      "('Epoch :', 58, 0.69333190302695, 0.4737903225806452)\n",
      "('Epoch :', 59, 0.69493148403783, 0.5020161290322581)\n",
      "('Epoch :', 60, 0.6933364079844567, 0.5100806451612904)\n",
      "('Epoch :', 61, 0.694606731014867, 0.4939516129032258)\n",
      "('Epoch :', 62, 0.6954350567633106, 0.49193548387096775)\n",
      "('Epoch :', 63, 0.6933215260505676, 0.5040322580645161)\n",
      "('Epoch :', 64, 0.6935690103038665, 0.46975806451612906)\n",
      "('Epoch :', 65, 0.6933636396161972, 0.5040322580645161)\n",
      "('Epoch :', 66, 0.6927752956267326, 0.5100806451612904)\n",
      "('Epoch :', 67, 0.692253178165805, 0.5241935483870968)\n",
      "('Epoch :', 68, 0.6947750929863222, 0.5181451612903226)\n",
      "('Epoch :', 69, 0.6926898821707694, 0.5463709677419355)\n",
      "('Epoch :', 70, 0.6954958650373644, 0.5443548387096774)\n",
      "('Epoch :', 71, 0.6946842074394226, 0.4939516129032258)\n",
      "('Epoch :', 72, 0.6956107443378817, 0.47580645161290325)\n",
      "('Epoch :', 73, 0.6933544489645189, 0.5221774193548387)\n",
      "('Epoch :', 74, 0.6939502531482328, 0.4536290322580645)\n",
      "('Validation :', 74, 0.6946518329473642, 0.44006410256410255)\n",
      "('Epoch :', 75, 0.6935592928240376, 0.5020161290322581)\n",
      "('Epoch :', 76, 0.6933096820308317, 0.5362903225806451)\n",
      "('Epoch :', 77, 0.694489682874372, 0.4838709677419355)\n",
      "('Epoch :', 78, 0.6935444589584104, 0.5060483870967742)\n",
      "('Epoch :', 79, 0.6935076502061659, 0.4959677419354839)\n",
      "('Epoch :', 80, 0.6943115265138687, 0.4637096774193548)\n",
      "('Epoch :', 81, 0.694084221316922, 0.48185483870967744)\n",
      "('Epoch :', 82, 0.6934534330521861, 0.5141129032258065)\n",
      "('Epoch :', 83, 0.6938993776998212, 0.5403225806451613)\n",
      "('Epoch :', 84, 0.6936988253747264, 0.4879032258064516)\n",
      "('Epoch :', 85, 0.6927776278988007, 0.5362903225806451)\n",
      "('Epoch :', 86, 0.6950691226989992, 0.49798387096774194)\n",
      "('Epoch :', 87, 0.6931951699718353, 0.4939516129032258)\n",
      "('Epoch :', 88, 0.6927682603559187, 0.5080645161290323)\n",
      "('Epoch :', 89, 0.6922655567046134, 0.5342741935483871)\n",
      "('Epoch :', 90, 0.693634856131769, 0.4838709677419355)\n",
      "('Epoch :', 91, 0.6954093498568381, 0.48588709677419356)\n",
      "('Epoch :', 92, 0.6928600918862128, 0.5080645161290323)\n",
      "('Epoch :', 93, 0.6927135548283977, 0.5342741935483871)\n",
      "('Epoch :', 94, 0.6945833102349313, 0.4838709677419355)\n",
      "('Epoch :', 95, 0.6934005656550007, 0.5443548387096774)\n",
      "('Epoch :', 96, 0.693114323000754, 0.5403225806451613)\n",
      "('Epoch :', 97, 0.6930246564649767, 0.4879032258064516)\n",
      "('Epoch :', 98, 0.6938059714532667, 0.46169354838709675)\n",
      "('Epoch :', 99, 0.6936741951973208, 0.5221774193548387)\n",
      "('Validation :', 99, 0.6934958696365356, 0.43846153846153846)\n",
      "('Epoch :', 100, 0.6943808274884378, 0.4798387096774194)\n",
      "('Epoch :', 101, 0.6927164985287574, 0.5161290322580645)\n",
      "('Epoch :', 102, 0.6896456787663121, 0.5)\n",
      "('Epoch :', 103, 0.6935731191788951, 0.49798387096774194)\n",
      "('Epoch :', 104, 0.6952311608099169, 0.4939516129032258)\n",
      "('Epoch :', 105, 0.6936364808390217, 0.5141129032258065)\n",
      "('Epoch :', 106, 0.6937544480446847, 0.5060483870967742)\n",
      "('Epoch :', 107, 0.6930636263662769, 0.5040322580645161)\n",
      "('Epoch :', 108, 0.6933734897644289, 0.49193548387096775)\n",
      "('Epoch :', 109, 0.6930809386314885, 0.4637096774193548)\n",
      "('Epoch :', 110, 0.6939532429941239, 0.5040322580645161)\n",
      "('Epoch :', 111, 0.69374874138063, 0.48588709677419356)\n",
      "('Epoch :', 112, 0.6933471848887782, 0.5443548387096774)\n",
      "('Epoch :', 113, 0.6936870198095998, 0.5120967741935484)\n",
      "('Epoch :', 114, 0.6931877886095354, 0.5020161290322581)\n",
      "('Epoch :', 115, 0.6923998882693629, 0.49798387096774194)\n",
      "('Epoch :', 116, 0.6923427293377538, 0.5080645161290323)\n",
      "('Epoch :', 117, 0.6943265103524731, 0.4959677419354839)\n",
      "('Epoch :', 118, 0.6923569767705856, 0.5201612903225806)\n",
      "('Epoch :', 119, 0.6942729911496562, 0.5060483870967742)\n",
      "('Epoch :', 120, 0.6936088089020022, 0.5241935483870968)\n",
      "('Epoch :', 121, 0.6930870259961774, 0.5020161290322581)\n",
      "('Epoch :', 122, 0.6940058027544329, 0.5)\n",
      "('Epoch :', 123, 0.6919998584255096, 0.5181451612903226)\n",
      "('Epoch :', 124, 0.6944832282681619, 0.5362903225806451)\n",
      "('Validation :', 124, 0.6885496286245493, 0.5798076923076922)\n",
      "('Epoch :', 125, 0.6945398084578975, 0.4798387096774194)\n",
      "('Epoch :', 126, 0.6940746653464532, 0.46975806451612906)\n",
      "('Epoch :', 127, 0.693714068781945, 0.49798387096774194)\n",
      "('Epoch :', 128, 0.694214517070401, 0.46774193548387094)\n",
      "('Epoch :', 129, 0.6930687100656571, 0.5040322580645161)\n",
      "('Epoch :', 130, 0.6903218300111832, 0.5221774193548387)\n",
      "('Epoch :', 131, 0.6954014397436573, 0.5080645161290323)\n",
      "('Epoch :', 132, 0.6927746822757106, 0.5524193548387096)\n",
      "('Epoch :', 133, 0.6928849393321622, 0.4879032258064516)\n",
      "('Epoch :', 134, 0.6929089657721981, 0.4596774193548387)\n",
      "('Epoch :', 135, 0.694781736020119, 0.4899193548387097)\n",
      "('Epoch :', 136, 0.6933171864478819, 0.4879032258064516)\n",
      "('Epoch :', 137, 0.6934528370057383, 0.46774193548387094)\n",
      "('Epoch :', 138, 0.6932444014856892, 0.4939516129032258)\n",
      "('Epoch :', 139, 0.6933814133367231, 0.5100806451612904)\n",
      "('Epoch :', 140, 0.6952375692705954, 0.4879032258064516)\n",
      "('Epoch :', 141, 0.6944044693823783, 0.5120967741935484)\n",
      "('Epoch :', 142, 0.6929061489720498, 0.5100806451612904)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch :', 143, 0.6946594388254227, 0.5100806451612904)\n",
      "('Epoch :', 144, 0.6923031326263182, 0.4939516129032258)\n",
      "('Epoch :', 145, 0.6958170686998675, 0.4576612903225806)\n",
      "('Epoch :', 146, 0.6938704540652614, 0.4737903225806452)\n",
      "('Epoch :', 147, 0.693901740735577, 0.5141129032258065)\n",
      "('Epoch :', 148, 0.6934036939374862, 0.5020161290322581)\n",
      "('Epoch :', 149, 0.6939269438866646, 0.5262096774193549)\n",
      "('Validation :', 149, 0.6925579538712134, 0.532051282051282)\n",
      "('Epoch :', 150, 0.6922417763740786, 0.5221774193548387)\n",
      "('Epoch :', 151, 0.6913459031812607, 0.4939516129032258)\n",
      "('Epoch :', 152, 0.6943096307016188, 0.5161290322580645)\n",
      "('Epoch :', 153, 0.6915599799925282, 0.4879032258064516)\n",
      "('Epoch :', 154, 0.6941882045038285, 0.5100806451612904)\n",
      "('Epoch :', 155, 0.6941225663308175, 0.5100806451612904)\n",
      "('Epoch :', 156, 0.6928735317722443, 0.5080645161290323)\n",
      "('Epoch :', 157, 0.6932165007437429, 0.5040322580645161)\n",
      "('Epoch :', 158, 0.6928886367428687, 0.4637096774193548)\n",
      "('Epoch :', 159, 0.6940688952322929, 0.5161290322580645)\n",
      "('Epoch :', 160, 0.6939051112821025, 0.4879032258064516)\n",
      "('Epoch :', 161, 0.693473594803964, 0.5141129032258065)\n",
      "('Epoch :', 162, 0.6927557395350549, 0.530241935483871)\n",
      "('Epoch :', 163, 0.6939436312644712, 0.48185483870967744)\n",
      "('Epoch :', 164, 0.6935661685082221, 0.4596774193548387)\n",
      "('Epoch :', 165, 0.6936620762271266, 0.4959677419354839)\n",
      "('Epoch :', 166, 0.6933581233024597, 0.5161290322580645)\n",
      "('Epoch :', 167, 0.6933566927909851, 0.4798387096774194)\n",
      "('Epoch :', 168, 0.6932059814853053, 0.4717741935483871)\n",
      "('Epoch :', 169, 0.6930057771744267, 0.46169354838709675)\n",
      "('Epoch :', 170, 0.6942915685715214, 0.5120967741935484)\n",
      "('Epoch :', 171, 0.6929343804236381, 0.5020161290322581)\n",
      "('Epoch :', 172, 0.6940352609080653, 0.46774193548387094)\n",
      "('Epoch :', 173, 0.6922752068888757, 0.5141129032258065)\n",
      "('Epoch :', 174, 0.6929815027021593, 0.5141129032258065)\n",
      "('Validation :', 174, 0.6911621919045081, 0.5217948717948718)\n",
      "('Epoch :', 175, 0.6929342900553057, 0.5362903225806451)\n",
      "('Epoch :', 176, 0.6939977888138064, 0.48185483870967744)\n",
      "('Epoch :', 177, 0.6919812021716949, 0.4879032258064516)\n",
      "('Epoch :', 178, 0.6933375904636998, 0.5120967741935484)\n",
      "('Epoch :', 179, 0.6948763574323347, 0.4899193548387097)\n",
      "('Epoch :', 180, 0.6933807134628296, 0.530241935483871)\n",
      "('Epoch :', 181, 0.6934329809681061, 0.5100806451612904)\n",
      "('Epoch :', 182, 0.6933493095059549, 0.48588709677419356)\n",
      "('Epoch :', 183, 0.694296085065411, 0.5282258064516129)\n",
      "('Epoch :', 184, 0.6933559506170212, 0.5080645161290323)\n",
      "('Epoch :', 185, 0.6933768641564154, 0.4899193548387097)\n",
      "('Epoch :', 186, 0.693517888745954, 0.49193548387096775)\n",
      "('Epoch :', 187, 0.6931812417122626, 0.4717741935483871)\n",
      "('Epoch :', 188, 0.692218428657901, 0.4657258064516129)\n",
      "('Epoch :', 189, 0.6935894969970949, 0.532258064516129)\n",
      "('Epoch :', 190, 0.6936148212802026, 0.4959677419354839)\n",
      "('Epoch :', 191, 0.6930817692510544, 0.5120967741935484)\n",
      "('Epoch :', 192, 0.6922767181550303, 0.5241935483870968)\n",
      "('Epoch :', 193, 0.6930785275274708, 0.5060483870967742)\n",
      "('Epoch :', 194, 0.6946730729072325, 0.49193548387096775)\n",
      "('Epoch :', 195, 0.6931985136001341, 0.5705645161290323)\n",
      "('Epoch :', 196, 0.6932429940469803, 0.5020161290322581)\n",
      "('Epoch :', 197, 0.6930217127646169, 0.5120967741935484)\n",
      "('Epoch :', 198, 0.694063824992026, 0.47580645161290325)\n",
      "('Epoch :', 199, 0.6934179875158495, 0.5020161290322581)\n",
      "('Validation :', 199, 0.6938762297997108, 0.48205128205128206)\n",
      "('Epoch :', 200, 0.6934333828187758, 0.47580645161290325)\n",
      "('Epoch :', 201, 0.6930365100983651, 0.5020161290322581)\n",
      "('Epoch :', 202, 0.6935058236122131, 0.5100806451612904)\n",
      "('Epoch :', 203, 0.691541402570663, 0.5282258064516129)\n",
      "('Epoch :', 204, 0.6939071186127201, 0.4657258064516129)\n",
      "('Epoch :', 205, 0.6934327079403785, 0.5403225806451613)\n",
      "('Epoch :', 206, 0.692741447879422, 0.5)\n",
      "('Epoch :', 207, 0.6939510703086853, 0.47580645161290325)\n",
      "('Epoch :', 208, 0.6924456088773666, 0.5423387096774194)\n",
      "('Epoch :', 209, 0.6916413787872561, 0.5282258064516129)\n",
      "('Epoch :', 210, 0.6934624583490433, 0.4737903225806452)\n",
      "('Epoch :', 211, 0.6937582069827665, 0.5161290322580645)\n",
      "('Epoch :', 212, 0.6935455972148526, 0.4657258064516129)\n",
      "('Epoch :', 213, 0.6931333022732888, 0.48588709677419356)\n",
      "('Epoch :', 214, 0.6933316434583356, 0.5120967741935484)\n",
      "('Epoch :', 215, 0.6931435792676864, 0.5282258064516129)\n",
      "('Epoch :', 216, 0.6933916134219016, 0.48588709677419356)\n",
      "('Epoch :', 217, 0.6946093324692019, 0.5040322580645161)\n",
      "('Epoch :', 218, 0.6928422547155811, 0.4939516129032258)\n",
      "('Epoch :', 219, 0.693477742133602, 0.4959677419354839)\n",
      "('Epoch :', 220, 0.6935529901135352, 0.4879032258064516)\n",
      "('Epoch :', 221, 0.6931914437201715, 0.4737903225806452)\n",
      "('Epoch :', 222, 0.6932840924109182, 0.4717741935483871)\n",
      "('Epoch :', 223, 0.6932096885096642, 0.4798387096774194)\n",
      "('Epoch :', 224, 0.6922381693317045, 0.5604838709677419)\n",
      "('Validation :', 224, 0.6927905587049631, 0.5352564102564102)\n",
      "('Epoch :', 225, 0.69291768727764, 0.5362903225806451)\n",
      "('Epoch :', 226, 0.6942382666372484, 0.4798387096774194)\n",
      "('Epoch :', 227, 0.693996129497405, 0.4657258064516129)\n",
      "('Epoch :', 228, 0.6931205295747326, 0.5120967741935484)\n",
      "('Epoch :', 229, 0.6931421949017432, 0.5141129032258065)\n",
      "('Epoch :', 230, 0.6931643755205216, 0.4717741935483871)\n",
      "('Epoch :', 231, 0.6935953363295524, 0.4717741935483871)\n",
      "('Epoch :', 232, 0.693125730560672, 0.4657258064516129)\n",
      "('Epoch :', 233, 0.6932138954439471, 0.49193548387096775)\n",
      "('Epoch :', 234, 0.6933990774616119, 0.4939516129032258)\n",
      "('Epoch :', 235, 0.6936292744451954, 0.4879032258064516)\n",
      "('Epoch :', 236, 0.6931820742545589, 0.5383064516129032)\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer_align = optim.Adam(model_align.parameters(), lr = 1e-5)\n",
    "for epoch in range(150):\n",
    "    accs = []\n",
    "    losses = []\n",
    "    model_align.train()\n",
    "    for batch_idx, (images, sounds, labels) in enumerate(train_loader):\n",
    "        images_v = Variable(images.type(torch.FloatTensor)).cuda()\n",
    "        sounds_v = Variable(sounds.type(torch.FloatTensor)).cuda()\n",
    "        labels_v = Variable(labels).cuda()\n",
    "        \n",
    "        optimizer_align.zero_grad()\n",
    "        aligned_res, _ = model_align(sounds_v, images_v)\n",
    "        loss = loss_fn(aligned_res, labels_v)\n",
    "        loss.backward()\n",
    "        optimizer_align.step()\n",
    "        losses.append(loss.item())\n",
    "        accs.append(np.mean((torch.argmax(aligned_res,1) == labels_v).detach().cpu().numpy()))\n",
    "    print(\"Epoch :\", epoch, np.mean(losses), np.mean(accs))\n",
    "    if (epoch + 1)%25 == 0:\n",
    "        accs = []\n",
    "        losses = []\n",
    "        model_align.eval()\n",
    "        for batch_idx, (images, sounds, labels) in enumerate(test_loader):\n",
    "            with torch.no_grad():\n",
    "                images_v = Variable(images.type(torch.FloatTensor)).cuda()\n",
    "                sounds_v = Variable(sounds.type(torch.FloatTensor)).cuda()\n",
    "                labels_v = Variable(labels).cuda()\n",
    "                aligned_res, _ = model_align(sounds_v, images_v)\n",
    "                loss = loss_fn(aligned_res, labels_v)\n",
    "                losses.append(loss.item())\n",
    "                accs.append(np.mean((torch.argmax(aligned_res,1) == labels_v).detach().cpu().numpy()))\n",
    "        print(\"Validation :\", epoch, np.mean(losses), np.mean(accs))\n",
    "torch.save(model_align, 'larger_150.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Map Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(feature_map, weights, label):\n",
    "    output = np.zeros((224,224))\n",
    "    for i in range(512):\n",
    "        output += imresize(feature_map[i], (224,224))*weights[label,i]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model_align.state_dict().items():\n",
    "    if name =='fc.weight_v':\n",
    "        weight = param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, sounds, labels = train_dataset[37]\n",
    "images_v = Variable(torch.tensor(images)).type(torch.FloatTensor).cuda().unsqueeze(0)\n",
    "sounds_v = Variable(torch.tensor(sounds)).type(torch.FloatTensor).cuda().unsqueeze(0)\n",
    "labels_v = Variable(torch.tensor(labels)).cuda().unsqueeze(0)\n",
    "aligned_res, feature_maps = model_align(sounds_v, images_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = activation(feature_maps[0,:,0].detach().cpu().numpy(), weight.detach().cpu().numpy(),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output, cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(images[0,0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-367.36499567, -367.35861125, -367.14408086, ...,    8.636785  ,\n",
       "          8.67113671,    8.9101118 ])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
